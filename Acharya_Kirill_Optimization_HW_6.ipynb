{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 6\n",
    "\n",
    "Это домашнее задание по материалам второго семинаров. Дедлайн по отправке - 23:55 24 марта. \n",
    "\n",
    "Домашнее задание выполняется в этом же Jupyter Notebook'e и присылается мне на почту: __beznosikov.an@phystech.edu__.\n",
    "\n",
    "Решение каждой задачи необходимо поместить после её условия.\n",
    "\n",
    "Файл должен называться: Фамилия_Имя_Optimization_HW_6\n",
    "\n",
    "При полном запуске Вашего решения (Kernel -> Restart & Run All) все ячейки должны выполняться без ошибок. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 1\n",
    "\n",
    "Рассмотрим задачу минимизации:\n",
    "\\begin{equation}\n",
    "\\min_{x \\in \\mathbb{R}^d} f(x) = \\frac{1}{2}x^T A x - b^T x,\n",
    "\\end{equation}\n",
    "где в качестве матрицы $A$ - случайная матрица с $L =1000$, $\\mu = 1$, $d = 100$, $b$ - так же случайный вектор.\n",
    "\n",
    "__(а)__ Для такой задачи симулируем итерацию координатного спуска (смотри семинар). \n",
    "\n",
    "Для такого метода подберите шаг $\\gamma$ для наилучшей сходимости постройте график сходимости (по оси $y$ - критерий $\\| \\nabla f(x^k)\\|^2$, а по оси $x$ - число арифмитических операций при подсчете \"градиента\"). Постройте на этом же графике сходимость честного градиентного спуска c наилучшим шагом сходимости. Сделайте вывод о характере сходимости нового метода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_optimization as opt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1000\n",
    "mu = 1\n",
    "d = 100\n",
    "\n",
    "args = {}\n",
    "args['A'] = gen_A(d, mu, L)\n",
    "args['b'] = np.random.random_sample(d)\n",
    "x_0 = np.random.random_sample(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ":CSGD: \n",
    "- gamma_th = 1/(4*L*d)\n",
    "- gamma_exp\n",
    "\n",
    ":GD:\n",
    "- gamma = 1/L\n",
    "\n",
    ":Method of calculating the number of iterations: \n",
    "- From the function f_quad_grad_f_j from opt \n",
    "we understand that the number of the number of arithmetic\n",
    "operations is d*#times of calling f_quad_grad_f_j\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csgd_gamma_th(k, f, grad_f, x_k, x_true, args):\n",
    "    return 1/(4 * L * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csgd_gamma_th = opt.GradientOptimizer(f_quad, f_quad_grad, x_0, \n",
    "                                      criterium='||grad_f(x_k)||',\n",
    "                                             gamma_k=gamma_coord_sgd_const,\n",
    "                                             args=args, nabla_f_j=nabla_f_j,\n",
    "                                             use_coord_sgd=True)\n",
    "    _, iterations, errors, _ = coord_sgd.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = [i + 1 for i in range(n_iter)]\n",
    "SGD_points = opt.Grad_Optimizer(n_iter, A, lr, b, x_0, optimizer = \"SGD\")\n",
    "GD_points = opt.Grad_Optimizer(n_iter, A, lr, b, x_0, optimizer = \"GD\")\n",
    "SGD_criteria_points = opt.criteria_points(A, b, SGD_points, criteria_type = \"||grad_f||^2\")\n",
    "GD_criteria_points = opt.criteria_points(A, b, GD_points, criteria_type = \"||grad_f||^2\")\n",
    "y = [SGD_criteria_points, GD_criteria_points]\n",
    "label = [\"SGD\", \"GD\"]\n",
    "\n",
    "opt.plot_graphs(iterations, y, label, \"Comparison of SGD and GD\", logscale = True, criteria_type = \"||grad_f||^2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(б)__ В координатном спуске можно брать несколько координат вместо одной. Модифицируйте координнатный спуск, исходя из этого, пробуя разное число координат $b = 1, 5, 10, 20$, а также разные способы сэмплирования: все координатны независимо или зависимо, чтобы не было повторений в батче. Постройте графики аналогичные пункту (а). Сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2\n",
    "\n",
    "Рассмотрим задачу линейной регрессии на датасете mushrooms с $\\ell_2$-регуляризацией (коэффициент регуляризации равен $\\frac{L}{1000}$). \n",
    "\n",
    "__(а)__ Оцените $\\mu$ и $L$ для данной задачи. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(б)__ Реализуйте координатный SGD, SEGA для задачи линейно регрессии. Каким будете брать $b$? Как будете выбирать шаг и другие параметры методов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(в)__ Постройте графики сравнения всех методов: по оси $x$ используйте время или количество арифметических операций, по оси $y$ - $\\|\\nabla f(x^k)\\|$ или точность на тесте. Таким образом, всего 4 графика. Сделайте сравнение и вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Бонусные пункты__\n",
    "\n",
    "__(г)__ Попробуйте объеденить подходы SAGA/SVRG/L-SVRG и координатный SGD/SEGA (т.е. могут получиться пары SAGA + координатный, SVRG + SEGA и еще четыре других). Вам достаточно выбрать одну пару. Как при этом будет выглядеть итерация метода (запишите в явном виде)? Докажите сходимость данного подхода для $\\mu$-сильно выпуклой $f$ и $L$-гладкой $f_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(д)__ Реализуйте новый метод. Сравните его с двумя методами, которые были взяты за базу, на задаче линейной регресии. Способы сравнения методов придумайте самостоятельно. Сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ответ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
